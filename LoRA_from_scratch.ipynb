{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%reset -f"
      ],
      "metadata": {
        "id": "lgkyPleqM--0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y transformers\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejnLwC3qM7Ww",
        "outputId": "26122a82-6f8e-4bf7-9e2b-1829a9a3b53d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: transformers 4.39.0.dev0\n",
            "Uninstalling transformers-4.39.0.dev0:\n",
            "  Successfully uninstalled transformers-4.39.0.dev0\n",
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-uzd77x7l\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-uzd77x7l\n",
            "  Resolved https://github.com/huggingface/transformers to commit 50db7ca4e874e211dd18d9b9ee429f62ef7d7e8f\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0.dev0) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0.dev0) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0.dev0) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0.dev0) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0.dev0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0.dev0) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0.dev0) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0.dev0) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0.dev0) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0.dev0) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.0.dev0) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.0.dev0) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.39.0.dev0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.39.0.dev0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.39.0.dev0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.39.0.dev0) (2024.2.2)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.39.0.dev0-py3-none-any.whl size=8593863 sha256=dc34c5d4345aa9e223f0d30903103908c906436d7dabd8dd5f4c71607d5d3b55\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-dzhkwtmn/wheels/c0/14/d6/6c9a5582d2ac191ec0a483be151a4495fe1eb2a6706ca49f1b\n",
            "Successfully built transformers\n",
            "Installing collected packages: transformers\n",
            "Successfully installed transformers-4.39.0.dev0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flYXeSGmMltx",
        "outputId": "a7f369a1-bf51-48f1-c11c-68199b5d0a7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return self.fget.__get__(instance, owner)()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "def print_prime(n):\n",
            "   \"\"\"\n",
            "   Print all primes between 1 and n\n",
            "   \"\"\"\n",
            "   primes = []\n",
            "   for num in range(2, n+1):\n",
            "       is_prime = True\n",
            "       for i in range(2, int(math.sqrt(num))+1):\n",
            "           if num % i == 0:\n",
            "               is_prime = False\n",
            "               break\n",
            "       if is_prime:\n",
            "           primes.append(num)\n",
            "   print(primes)\n",
            "   \n",
            "print_prime(20)\n",
            "```\n",
            "\n",
            "Output:\n",
            "```\n",
            "[2, 3, 5, 7, 11, 13, 17, 19]\n",
            "```\n",
            "\n",
            "Exercise 5:\n",
            "Write a Python function that takes a list of numbers and returns the sum of all even numbers in the list.\n",
            "\n",
            "```python\n",
            "def sum_even(numbers):\n",
            "   \"\"\"\n",
            "   \n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "torch.set_default_device(\"cuda\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1_5\", torch_dtype=\"auto\", trust_remote_code=True)\n",
        "model_lora=AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1_5\", torch_dtype=\"auto\", trust_remote_code=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True)\n",
        "\n",
        "inputs = tokenizer('''def print_prime(n):\n",
        "   \"\"\"\n",
        "   Print all primes between 1 and n\n",
        "   \"\"\"''', return_tensors=\"pt\", return_attention_mask=False)\n",
        "\n",
        "outputs = model.generate(**inputs, max_length=200)\n",
        "text = tokenizer.batch_decode(outputs)[0]\n",
        "print(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer('''Who did it?''', return_tensors=\"pt\", return_attention_mask=False)\n",
        "\n",
        "outputs = model.generate(**inputs, max_length=200)\n",
        "text = tokenizer.batch_decode(outputs)[0]"
      ],
      "metadata": {
        "id": "d1WNdV9JNuGs"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "irjGpHFVN6wY",
        "outputId": "00ee14a1-48d7-4857-8b4d-515ef0d44944"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Who did it? Who\\'s responsible?\"\\n\\nThe man looked up, startled. \"I didn\\'t do it,\" he said. \"I swear.\"\\n\\nThe woman shook her head. \"I don\\'t believe you. You\\'re lying.\"\\n\\nThe man\\'s face turned red. \"I\\'m not lying,\" he said. \"I\\'m telling the truth.\"\\n\\nThe woman wasn\\'t convinced. She decided to call the police and report the theft. As she waited for them to arrive, she couldn\\'t help but think about the man\\'s story. She wondered if he was telling the truth, or if he was just trying to cover up his crime.\\n\\nWhen the police arrived, they took the man\\'s statement and promised to investigate. The woman watched as they drove away, feeling a sense of unease. She couldn\\'t shake the feeling that something wasn\\'t right.\\n\\nAs she walked back to her apartment, she noticed a group of people gathered around a street'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for name, module in model.named_children():\n",
        "\n",
        "#     print(f\"Layer Name: {name}\\nModule: {module}\\n\")\n"
      ],
      "metadata": {
        "id": "0eDNbxkUN-er"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_linear_layers(module, parent_name=''):\n",
        "    for name, child in module.named_children():\n",
        "        # print(name,' ::: ',child)\n",
        "        if isinstance(child, torch.nn.Linear):\n",
        "            print(f\"Layer Name: {parent_name + '.' + name if parent_name else name}\\nModule: {child}\\n\")\n",
        "        else:\n",
        "            # print('rec call')\n",
        "            print_linear_layers(child, parent_name=name if not parent_name else parent_name + '.' + name)\n"
      ],
      "metadata": {
        "id": "V4njcS7wOwUM"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_linear_layers(model)"
      ],
      "metadata": {
        "id": "Hi-RQXq6PQjr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52e6518e-2906-41d1-87e1-5fa6d427046c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer Name: model.layers.0.self_attn.q_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.0.self_attn.k_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.0.self_attn.v_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.0.self_attn.dense\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.0.mlp.fc1\n",
            "Module: Linear(in_features=2048, out_features=8192, bias=True)\n",
            "\n",
            "Layer Name: model.layers.0.mlp.fc2\n",
            "Module: Linear(in_features=8192, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.1.self_attn.q_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.1.self_attn.k_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.1.self_attn.v_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.1.self_attn.dense\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.1.mlp.fc1\n",
            "Module: Linear(in_features=2048, out_features=8192, bias=True)\n",
            "\n",
            "Layer Name: model.layers.1.mlp.fc2\n",
            "Module: Linear(in_features=8192, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.2.self_attn.q_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.2.self_attn.k_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.2.self_attn.v_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.2.self_attn.dense\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.2.mlp.fc1\n",
            "Module: Linear(in_features=2048, out_features=8192, bias=True)\n",
            "\n",
            "Layer Name: model.layers.2.mlp.fc2\n",
            "Module: Linear(in_features=8192, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.3.self_attn.q_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.3.self_attn.k_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.3.self_attn.v_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.3.self_attn.dense\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.3.mlp.fc1\n",
            "Module: Linear(in_features=2048, out_features=8192, bias=True)\n",
            "\n",
            "Layer Name: model.layers.3.mlp.fc2\n",
            "Module: Linear(in_features=8192, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.4.self_attn.q_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.4.self_attn.k_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.4.self_attn.v_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.4.self_attn.dense\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.4.mlp.fc1\n",
            "Module: Linear(in_features=2048, out_features=8192, bias=True)\n",
            "\n",
            "Layer Name: model.layers.4.mlp.fc2\n",
            "Module: Linear(in_features=8192, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.5.self_attn.q_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.5.self_attn.k_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.5.self_attn.v_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.5.self_attn.dense\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.5.mlp.fc1\n",
            "Module: Linear(in_features=2048, out_features=8192, bias=True)\n",
            "\n",
            "Layer Name: model.layers.5.mlp.fc2\n",
            "Module: Linear(in_features=8192, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.6.self_attn.q_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.6.self_attn.k_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.6.self_attn.v_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.6.self_attn.dense\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.6.mlp.fc1\n",
            "Module: Linear(in_features=2048, out_features=8192, bias=True)\n",
            "\n",
            "Layer Name: model.layers.6.mlp.fc2\n",
            "Module: Linear(in_features=8192, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.7.self_attn.q_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.7.self_attn.k_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.7.self_attn.v_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.7.self_attn.dense\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.7.mlp.fc1\n",
            "Module: Linear(in_features=2048, out_features=8192, bias=True)\n",
            "\n",
            "Layer Name: model.layers.7.mlp.fc2\n",
            "Module: Linear(in_features=8192, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.8.self_attn.q_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.8.self_attn.k_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.8.self_attn.v_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.8.self_attn.dense\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.8.mlp.fc1\n",
            "Module: Linear(in_features=2048, out_features=8192, bias=True)\n",
            "\n",
            "Layer Name: model.layers.8.mlp.fc2\n",
            "Module: Linear(in_features=8192, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.9.self_attn.q_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.9.self_attn.k_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.9.self_attn.v_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.9.self_attn.dense\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.9.mlp.fc1\n",
            "Module: Linear(in_features=2048, out_features=8192, bias=True)\n",
            "\n",
            "Layer Name: model.layers.9.mlp.fc2\n",
            "Module: Linear(in_features=8192, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.10.self_attn.q_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.10.self_attn.k_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.10.self_attn.v_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.10.self_attn.dense\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.10.mlp.fc1\n",
            "Module: Linear(in_features=2048, out_features=8192, bias=True)\n",
            "\n",
            "Layer Name: model.layers.10.mlp.fc2\n",
            "Module: Linear(in_features=8192, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.11.self_attn.q_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.11.self_attn.k_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.11.self_attn.v_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.11.self_attn.dense\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.11.mlp.fc1\n",
            "Module: Linear(in_features=2048, out_features=8192, bias=True)\n",
            "\n",
            "Layer Name: model.layers.11.mlp.fc2\n",
            "Module: Linear(in_features=8192, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.12.self_attn.q_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.12.self_attn.k_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.12.self_attn.v_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.12.self_attn.dense\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.12.mlp.fc1\n",
            "Module: Linear(in_features=2048, out_features=8192, bias=True)\n",
            "\n",
            "Layer Name: model.layers.12.mlp.fc2\n",
            "Module: Linear(in_features=8192, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.13.self_attn.q_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.13.self_attn.k_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.13.self_attn.v_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.13.self_attn.dense\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.13.mlp.fc1\n",
            "Module: Linear(in_features=2048, out_features=8192, bias=True)\n",
            "\n",
            "Layer Name: model.layers.13.mlp.fc2\n",
            "Module: Linear(in_features=8192, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.14.self_attn.q_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.14.self_attn.k_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.14.self_attn.v_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.14.self_attn.dense\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.14.mlp.fc1\n",
            "Module: Linear(in_features=2048, out_features=8192, bias=True)\n",
            "\n",
            "Layer Name: model.layers.14.mlp.fc2\n",
            "Module: Linear(in_features=8192, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.15.self_attn.q_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.15.self_attn.k_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.15.self_attn.v_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.15.self_attn.dense\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.15.mlp.fc1\n",
            "Module: Linear(in_features=2048, out_features=8192, bias=True)\n",
            "\n",
            "Layer Name: model.layers.15.mlp.fc2\n",
            "Module: Linear(in_features=8192, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.16.self_attn.q_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.16.self_attn.k_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.16.self_attn.v_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.16.self_attn.dense\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.16.mlp.fc1\n",
            "Module: Linear(in_features=2048, out_features=8192, bias=True)\n",
            "\n",
            "Layer Name: model.layers.16.mlp.fc2\n",
            "Module: Linear(in_features=8192, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.17.self_attn.q_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.17.self_attn.k_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.17.self_attn.v_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.17.self_attn.dense\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.17.mlp.fc1\n",
            "Module: Linear(in_features=2048, out_features=8192, bias=True)\n",
            "\n",
            "Layer Name: model.layers.17.mlp.fc2\n",
            "Module: Linear(in_features=8192, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.18.self_attn.q_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.18.self_attn.k_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.18.self_attn.v_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.18.self_attn.dense\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.18.mlp.fc1\n",
            "Module: Linear(in_features=2048, out_features=8192, bias=True)\n",
            "\n",
            "Layer Name: model.layers.18.mlp.fc2\n",
            "Module: Linear(in_features=8192, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.19.self_attn.q_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.19.self_attn.k_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.19.self_attn.v_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.19.self_attn.dense\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.19.mlp.fc1\n",
            "Module: Linear(in_features=2048, out_features=8192, bias=True)\n",
            "\n",
            "Layer Name: model.layers.19.mlp.fc2\n",
            "Module: Linear(in_features=8192, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.20.self_attn.q_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.20.self_attn.k_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.20.self_attn.v_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.20.self_attn.dense\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.20.mlp.fc1\n",
            "Module: Linear(in_features=2048, out_features=8192, bias=True)\n",
            "\n",
            "Layer Name: model.layers.20.mlp.fc2\n",
            "Module: Linear(in_features=8192, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.21.self_attn.q_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.21.self_attn.k_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.21.self_attn.v_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.21.self_attn.dense\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.21.mlp.fc1\n",
            "Module: Linear(in_features=2048, out_features=8192, bias=True)\n",
            "\n",
            "Layer Name: model.layers.21.mlp.fc2\n",
            "Module: Linear(in_features=8192, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.22.self_attn.q_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.22.self_attn.k_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.22.self_attn.v_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.22.self_attn.dense\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.22.mlp.fc1\n",
            "Module: Linear(in_features=2048, out_features=8192, bias=True)\n",
            "\n",
            "Layer Name: model.layers.22.mlp.fc2\n",
            "Module: Linear(in_features=8192, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.23.self_attn.q_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.23.self_attn.k_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.23.self_attn.v_proj\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.23.self_attn.dense\n",
            "Module: Linear(in_features=2048, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: model.layers.23.mlp.fc1\n",
            "Module: Linear(in_features=2048, out_features=8192, bias=True)\n",
            "\n",
            "Layer Name: model.layers.23.mlp.fc2\n",
            "Module: Linear(in_features=8192, out_features=2048, bias=True)\n",
            "\n",
            "Layer Name: lm_head\n",
            "Module: Linear(in_features=2048, out_features=51200, bias=True)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "lora_model = copy.deepcopy(model)"
      ],
      "metadata": {
        "id": "sIr1e-S7TiBY"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LoRALayer(torch.nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
        "        super().__init__()\n",
        "        std_dev = 1 / torch.sqrt(torch.tensor(rank).float())\n",
        "        self.A = torch.nn.Parameter(torch.randn(in_dim, rank) * std_dev)\n",
        "        self.B = torch.nn.Parameter(torch.zeros(rank, out_dim))\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.alpha * (x @ self.A @ self.B)\n",
        "        return x"
      ],
      "metadata": {
        "id": "IGYH1VBYUlVt"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearWithLoRA(torch.nn.Module):\n",
        "    def __init__(self, linear, rank, alpha):\n",
        "        super().__init__()\n",
        "        self.linear = linear\n",
        "        self.lora = LoRALayer(\n",
        "            linear.in_features, linear.out_features, rank, alpha\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x) + self.lora(x)"
      ],
      "metadata": {
        "id": "IubTBd2-VIS5"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_linear_with_lora(model, rank, alpha):\n",
        "    for name, child in model.named_children():\n",
        "        if isinstance(child, torch.nn.Linear):\n",
        "            setattr(model, name, LinearWithLoRA(child, rank, alpha))\n",
        "        else:\n",
        "            replace_linear_with_lora(child, rank, alpha)\n"
      ],
      "metadata": {
        "id": "0R9mEZp-VRS4"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "replace_linear_with_lora(lora_model,8,16)"
      ],
      "metadata": {
        "id": "NzMjKmSbVZb1"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)\n",
        "print(\"------------------------------------LoRA Model--------------------------------\")\n",
        "print(lora_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5882UieLWHda",
        "outputId": "fc770d11-b881-449d-b44f-5ec875313779"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PhiForCausalLM(\n",
            "  (model): PhiModel(\n",
            "    (embed_tokens): Embedding(51200, 2048)\n",
            "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
            "    (layers): ModuleList(\n",
            "      (0-23): 24 x PhiDecoderLayer(\n",
            "        (self_attn): PhiAttention(\n",
            "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
            "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
            "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
            "          (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
            "          (rotary_emb): PhiRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): PhiMLP(\n",
            "          (activation_fn): NewGELUActivation()\n",
            "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
            "          (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
            "        )\n",
            "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (final_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=2048, out_features=51200, bias=True)\n",
            ")\n",
            "------------------------------------LoRA Model--------------------------------\n",
            "PhiForCausalLM(\n",
            "  (model): PhiModel(\n",
            "    (embed_tokens): Embedding(51200, 2048)\n",
            "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
            "    (layers): ModuleList(\n",
            "      (0-23): 24 x PhiDecoderLayer(\n",
            "        (self_attn): PhiAttention(\n",
            "          (q_proj): LinearWithLoRA(\n",
            "            (linear): Linear(in_features=2048, out_features=2048, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "          (k_proj): LinearWithLoRA(\n",
            "            (linear): Linear(in_features=2048, out_features=2048, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "          (v_proj): LinearWithLoRA(\n",
            "            (linear): Linear(in_features=2048, out_features=2048, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "          (dense): LinearWithLoRA(\n",
            "            (linear): Linear(in_features=2048, out_features=2048, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "          (rotary_emb): PhiRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): PhiMLP(\n",
            "          (activation_fn): NewGELUActivation()\n",
            "          (fc1): LinearWithLoRA(\n",
            "            (linear): Linear(in_features=2048, out_features=8192, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "          (fc2): LinearWithLoRA(\n",
            "            (linear): Linear(in_features=8192, out_features=2048, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "        )\n",
            "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (final_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): LinearWithLoRA(\n",
            "    (linear): Linear(in_features=2048, out_features=51200, bias=True)\n",
            "    (lora): LoRALayer()\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5J_DmTTjWULl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}